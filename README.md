# ğŸ¤– CreatorAssistant: Fine-Tuned LLM Chatbot for Creator Economy

End-to-end PyTorch implementation of a conversational AI chatbot fine-tuned for creator-fan interactions.

## ğŸ¯ Project Overview

This project demonstrates:
- Fine-tuning LLMs with **LoRA/QLoRA** on custom datasets
- Building production-ready **inference API** with FastAPI
- Creating interactive **UI** with Streamlit
- Implementing **evaluation pipeline** for model quality

## ğŸ› ï¸ Tech Stack

- **Model**: Llama-2-7B-chat
- **Framework**: PyTorch + Transformers + PEFT
- **Training**: QLoRA (4-bit quantization)
- **API**: FastAPI
- **UI**: Streamlit
- **Dataset**: Bitext Customer Support (adapted for creator economy)

## ğŸ“Š Results

- **Training samples**: ~24,000
- **Training time**: ~3 hours on T4 GPU
- **Model size**: ~3GB (with LoRA adapters)
- **Inference speed**: ~2-3 sec per response

## ğŸš€ Quick Start

### 1. Install dependencies
```bash
pip install -r requirements.txt
```

### 2. Prepare data
```bash
python data/prepare_data.py
```

### 3. Train model
```bash
python training/train.py
```

**Note**: Training requires a GPU. You can use:
- Google Colab (free T4 GPU)
- Kaggle Notebooks (free GPU)
- Local GPU (NVIDIA with CUDA support)

Expected training time: 2-4 hours on T4 GPU

### 4. Run API server
```bash
python inference/app.py
```

The API will be available at `http://localhost:8000`

### 5. Launch UI
```bash
streamlit run ui/streamlit_app.py
```

The Streamlit interface will open in your browser at `http://localhost:8501`

## ğŸ“ Project Structure

```
CreatorAssistant/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config/
â”‚   â””â”€â”€ training_config.yaml        # Training hyperparameters
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                        # Original dataset (auto-downloaded)
â”‚   â”œâ”€â”€ processed/                  # Processed train/eval splits
â”‚   â”‚   â”œâ”€â”€ train.json
â”‚   â”‚   â””â”€â”€ eval.json
â”‚   â””â”€â”€ prepare_data.py             # Data preparation script
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ train.py                    # Main training script
â”‚   â”œâ”€â”€ model_config.py             # Model configuration classes
â”‚   â””â”€â”€ utils.py                    # Training utilities
â”œâ”€â”€ inference/
â”‚   â”œâ”€â”€ app.py                      # FastAPI server
â”‚   â”œâ”€â”€ chat_handler.py             # Chat history management
â”‚   â””â”€â”€ model_loader.py             # Model loading utilities
â”œâ”€â”€ ui/
â”‚   â””â”€â”€ streamlit_app.py            # Streamlit interface
â”œâ”€â”€ evaluation/
â”‚   â”œâ”€â”€ evaluate.py                 # Evaluation script
â”‚   â”œâ”€â”€ test_prompts.json           # Test questions
â”‚   â””â”€â”€ results/                    # Evaluation results
â”‚       â””â”€â”€ comparison.csv
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ base/                       # Base model (auto-downloaded)
â”‚   â””â”€â”€ finetuned/                  # Fine-tuned model with LoRA adapters
â””â”€â”€ logs/
    â””â”€â”€ training.log                # Training logs
```

## ğŸ”§ Configuration

Edit `config/training_config.yaml` to customize:

- **Model selection**: Choose between Llama-2, Mistral, or other models
- **LoRA parameters**: Adjust `r`, `lora_alpha`, `target_modules`
- **Training hyperparameters**: Batch size, learning rate, epochs, etc.

## ğŸ“š API Usage

### Health Check
```bash
curl http://localhost:8000/health
```

### Chat Endpoint
```bash
curl -X POST "http://localhost:8000/chat" \
  -H "Content-Type: application/json" \
  -d '{
    "message": "How do I cancel my membership?",
    "temperature": 0.7,
    "max_length": 256
  }'
```

Response:
```json
{
  "response": "To cancel your membership, please visit your account settings..."
}
```

## ğŸ§ª Evaluation

Run evaluation on test prompts:

```bash
python evaluation/evaluate.py
```

This will:
1. Load the fine-tuned model
2. Test it on predefined prompts
3. Generate response statistics
4. Save results to `evaluation/results/comparison.csv`

## ğŸ“ Key Learnings

- âœ… Implemented LoRA for parameter-efficient fine-tuning
- âœ… Built end-to-end ML pipeline from data to deployment
- âœ… Optimized inference for production use
- âœ… Created evaluation framework for LLM quality

## ğŸ”„ Next Steps

Potential improvements:
- [ ] Add RAG (Retrieval-Augmented Generation) for knowledge base
- [ ] Implement conversation memory across sessions
- [ ] Add user authentication to the API
- [ ] Deploy to cloud (AWS/GCP/Azure)
- [ ] Add response quality metrics (BLEU, ROUGE, etc.)
- [ ] Implement A/B testing framework
- [ ] Add multi-language support

## ğŸ“ License

This project is for educational purposes.

## ğŸ¤ Contributing

Feel free to open issues or submit pull requests!

## ğŸ“§ Contact

For questions or suggestions, please open an issue on GitHub.

---

Made with â¤ï¸ for the Creator Economy
